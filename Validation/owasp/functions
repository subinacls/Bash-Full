################################################################################
#
# OWASP Testing and Validation Functions for Bash
#
#### Review Webserver Metafiles for Information Leakage (OTG-INFO-003)
# grabs the robots.txt file from "/" dir of web site
# saves output to dir on disk for later consumption
#
robotears() {
  if [ ! -d "./owasp/" ]; then \
    mkdir ./owasp >/dev/null;
  fi;
  if [ ! -d "./owasp/robots/" ]; then \
    mkdir ./owasp/robots >/dev/null;
  fi;
  # 
  wget --no-check-certificate \
    --user-agent="NO UA" $1/robots.txt \
    -O ./robots/`echo $1`_robots.txt 2>/dev/null;
  echo -e "\n" >> ./owasp/robots/`echo $1`_robots.txt;
}  # robotears site.tld
#
#### Conduct search engine discovery/reconnaissance for information leakage (OTG-INFO-001)
# This function does numerous searches across different servers to gather information leaks
# Uses Shodan and other sites that scan and archive services over time
# 
scrapeallinfo() {
  # DROPPING IN FUTURE UPDATES
} # infogather site.tld
#
####
# This test function grabs the Headers of the HTTP server / follows redirections
# further grabbing headers as they are presented to the crawler
# 
spidereye() {
  if [ ! -d "./owasp/" ]; then \
    mkdir ./owasp >/dev/null;
  fi;
  if [ ! -d "./owasp/spidereyes/" ]; then \
    mkdir ./owasp/spidereyes >/dev/null;
  fi;
  #
  wget -T 5 -t 1 \
    --no-check-certificate \
    --server-response \
    --spider --user-agent="Unknown UA" \
    http://$1 2>./owasp/spidereyes/`echo $1`_server_headers.txt 3>/dev/null;
  echo -e "\n" >> ./owasp/spidereyes/`echo $1`_server_headers.txt;
} # spidereye site.tld
#
####
# This test function grabs the Headers of the HTTPS server / follows redirections
# further grabbing headers as they are presented to the crawler
# 
spidereyes() {
  if [ ! -d "./owasp/" ]; then \
    mkdir ./owasp >/dev/null;
  fi;
  if [ ! -d "./owasp/spidereyes/" ]; then \
    mkdir ./owasp/spidereyes >/dev/null;
  fi;
  #
  wget -T 5 -t 1 \
    --no-check-certificate \
    --server-response \
    --spider --user-agent="Unknown UA" \
    https://$1 2>./owasp/spidereyes/`echo $1`_server_headers.txt 3>/dev/null;
  echo -e "\n" >> ./owasp/spidereyes/`echo $1`_server_headers.txt;
} # spidereyes site.tld
#
####
# Bulk test function grabs the Headers of the HTTP server / follows redirections
# further grabbing headers as they are presented to the crawler
# 
HTTPspidereyes() {
  if [ ! -d "./owasp/" ]; then \
    mkdir ./owasp >/dev/null;
  fi;
  if [ ! -d "./owasp/spidereyes/" ]; then \
    mkdir ./owasp/spidereyes >/dev/null;
  fi;
  for x in $(cat $1); do \
    echo "Scanning HTTP Headers on: `echo $x`";
    wget -T 5 -t 1 \
      --server-response \
      --spider --user-agent="Unknown UA" \
      http://$x 2>./owasp/spidereyes/`echo $x`_HTTP_server_headers.txt 3>/dev/null;
    echo -e "\n" >> ./owasp/spidereyes/`echo $x`_HTTP_server_headers.txt;
  done;
} # HTTPspidereyes myDNShostlist#
####
# Bulk test function grabs the Headers of the HTTPS server / follows redirections
# further grabbing headers as they are presented to the crawler
# 
SSLspidereyes() {
  if [ ! -d "./owasp/" ]; then \
    mkdir ./owasp >/dev/null;
  fi;
  if [ ! -d "./owasp/spidereyes/" ]; then \
    mkdir ./owasp/spidereyes >/dev/null;
  fi;
  for x in $(cat $1); do \
    echo "Scanning HTTPS Headers on: `echo $x`";
    wget -T 5 -t 1 \
      --no-check-certificate \
      --server-response \
      --spider --user-agent="Unknown UA" \
      https://$x 2>./owasp/spidereyes/`echo $x`_SSL_server_headers.txt 3>/dev/null;
    echo -e "\n" >> ./owasp/spidereyes/`echo $x`_SSL_server_headers.txt;
  done;
} # SSLspidereyes myDNShostlist
#
####
# Grabs Server Header and displays to STDOUT
# Can be redirected to file for archiving with > filenamehere
# 
getservertype() {
  grep -i "server\:" ./owasp/spidereyes/* |\
  sed -r "s/\:  /\:\t/g";
} # getservertype / getservertype > filenamehere
#
####
# Grabs Set-Cookies Header and displays to STDOUT
# Can be redirected to file for archiving with > filenamehere
# 
getallcookies() {
  grep -i "set-cookie\:" ./owasp/spidereyes/*;
} # getallcookies / getallcookies > filenamehere
#
####
# Grabs Set-Cookies Header and displays only weak cookies to STDOUT
# Can be redirected to file for archiving with > filenamehere
# 
getweakcookies() {
  grep -i "set-cookie\:" ./owasp/spidereyes/* |\
  grep -Eiv "(secure|httponly)" |\
  sed -r "s/\:  /\:\t/g";
} # getweakcookies / getweakcookies > /filename
#
####
# Used to profile pages from a server and gather the URI/L removing the clutter 
# to easily identify unique entry points, can redirect output to a file
#
getresource() { 
  wget -T 5 -t 1 \
    --no-check-certificate \
    --spider --force-html -r -l2 $1 2>&1 |\
  grep '^--' |\
  awk '{ print $3 }' |\
  cut -d"?" -f1 |\
  grep -Evi "(\.jpg|\.png|\.css)" |\
  sort -u; 
} # getresource site.tld / getresource site.tld > filename
#
####
# Bulk function sed to profile pages from a DNS list to gather the URI/L removing the clutter 
# to easily identify unique entry points, can redirect output to a file
#
SSLgetresources() { 
  if [ ! -d "./owasp/" ]; then \
    mkdir ./owasp >/dev/null;
  fi;
  if [ ! -d "./owasp/getresources/" ]; then \
    mkdir ./owasp/getresources 2>/dev/null;
  fi;
  for x in $(cat $1); do \
    rm ./owasp/getresources/`echo $x`_resources 2>/dev/null;
    echo "Identifying URIs hosted from site: `echo $x`";
    wget -T 5 -t 1 --spider \
      --no-check-certificate \
      --force-html \
      -r -l2 https://$x 2>&1 |\
    grep '^--' |\
    awk '{ print $3 }' |\
    cut -d"?" -f1 |\
    grep -Evi "(\.jpg|\.gif|\.ico|\.png|\.css|\.pdf)" |\
    sort -u >> ./owasp/getresources/`echo $x`_resources;
  done;
} # SSLgetresources mydnslist
#
####
#
#
#
HTTPdownitall() {
  if [ ! -d "./owasp/" ]; then \
    mkdir ./owasp >/dev/null;
  fi;
  if [ ! -d "./owasp/webarchive/" ]; then \
    mkdir ./owasp/webarchive 2>/dev/null;
  fi;
  for x in $(cat $1); do \
    echo "Identifying URIs hosted from site: `echo $x`";
    wget -T 5 -t 1 --spider \
      --no-check-certificate \
      --force-html \
      -r -l2 https://$x 2>&1 |\
    grep '^--' |\
    awk '{ print $3 }' |\
    cut -d"?" -f1 |\
    sort -u >> ./owasp/getresources/`echo $x`_resources;
  done;
}
#
####
# Downloads HTTPS site in a recursive manner with no clobber
# stays within the domain and does not check for invalid certificates
#
HTTPSdownitall() { wget https://$1 \
      --recursive \
      --user-agent="Unknown UA" \
      --no-clobber \
      --no-check-certificate \
      --page-requisites \
      --html-extension \
      --convert-links \
      --domain $1 
      --no-parent \
      -t 1 -T 5 2>/dev/null;
}  # HTTPSdownitall google.com
#
####
# Downloads HTTP site in a recursive manner with no clobber
# stays within the domain and does not check for invalid certificates
#-e use_proxy=yes -e http_proxy=127.0.0.1:8080
HTTPdownitall() { wget http://$1 \
      --recursive \
      --user-agent="Unknown UA" \
      --no-clobber \
      --no-check-certificate \
      --page-requisites \
      --html-extension \
      --convert-links \
      --domain `echo $1 | cut -d":" -f1` 
      --no-parent \
      -t 1 -T 5 2>/dev/null;
}  # HTTPdownitall google.com
#
####
# Utilize BurpSuite - Downloads HTTPS site in a recursive manner with no clobber
# stays within the domain and does not check for invalid certificates
#
HTTPSBurpitall() { wget https://$1 \
      -e use_proxy=on \
      -e http_proxy=127.0.0.1:8080 \
      -e https_proxy=127.0.0.1:8080 \
      -e ftp_proxy=127.0.0.1:8080 \
      --recursive \
      --user-agent="Unknown UA" \
      --no-check-certificate \
      --page-requisites \
      --html-extension \
      --convert-links \
      --domain $1 \
      --no-parent \
      -t 1 -T 5 -4 2>/dev/null;
}  # HTTPSdownitall google.com#
####
# Utilize BurpSuite - Downloads HTTP site in a recursive manner with no clobber
# stays within the domain and does not check for invalid certificates
#-e use_proxy=yes -e http_proxy=127.0.0.1:8080
HTTPBurpitall() { wget http://$1 \
      -e use_proxy=on \
      -e http_proxy=127.0.0.1:8080 \
      -e https_proxy=127.0.0.1:8080 \
      -e ftp_proxy=127.0.0.1:8080 \
      --recursive \
      --user-agent="Unknown UA" \
      --no-check-certificate \
      --page-requisites \
      --html-extension \
      --convert-links \
      --domain `echo $1 | cut -d":" -f1 >/dev/null` \
      --no-parent \
      -t 1 -T 5 -4 2>/dev/null;
}  # HTTPdownitall google.com
